---
layout: post
title: 'Case study: building Kafka - Kafka Connect - PostgreSQL data pipeline with a REST API on Python for an airline'
date: '2024-05-18'
author: Gelassen
tags: 
modified_time: '2024-05-18'
---
<html>
    <head>
        <link rel="stylesheet" href=".css/general.css">
    </head>
    <body>
        <h3>Executive summary</h3>
        <p>
            Over passed week I learned a new tech stack and built a PoC for the airline 
            which is data pipeline for their stream of upcoming flights. The project is 
            based on Kafka, Kafka Connect and PostgreSQL with a REST server on 
            Python. Everything containerized in Docker and run deployed over 
            Docker Compose. <br><br>

            Apache Kafka is an open source solution for a message queue which is 
            designed to be robust, fault-tolerant and easily scaled up in 
            response to quick growth of upcoming data. <br>
            Kafka Connect is a solution to integrate Kafka with many data 
            sources and data storages by just providing configuration file. 
            Then Kafka would automatically upload and data to & out the queue 
            from & to specified source and sink databases. <br>
            PostgreSQL is just one more database which fits better my potential 
            customer needs. <br>
            
            Not Everything went smoothly. Kafka migrates from Zookeeper to KRaft 
            - internal mechanisms used by Kafka to manage its state - which 
            caused me some issues which caused me to pivot a project from pure 
            apache/kafka to confluentinc/kafka implementation which has an offer 
            of its code under acceptable open source license.  
            
            This is a first project where I had used ChatGPT to configure Kafka, 
            write backend on Python and also to write this publication. It speed 
            up process of learning a new tech 
            stack on at least 50%. It is good for search, it is very good in 
            refactoring code, it is code in code review. It is not good in 
            fully writing software, it could not (yet) fully replace your 
            experience. It still can not avoid mistakes. Despite on ChatGpt is 
            a good aid as an expert system for a software engineer, its future 
            impact on a software industry is hazy - it my cause engineers rely 
            more on technology rather than on their own knowledge and skills 
            which would cause degradation of their qualification.  
        </p>
        <h3>Whats is an Apache Kafka?</h3>
        <p>
            <h4>1. Kafka Architecture Overview</h4>
            <p>
                <pre>
                    Producers: Entities or applications that send data to Kafka. They publish messages to Kafka topics.
                    Kafka Cluster: The core of Kafka, consisting of multiple Kafka brokers.
                        Brokers: Individual servers that handle data storage, retrieval, and serve clients' requests.
                    Topics: Logical channels to which messages are sent. Each topic can have multiple partitions.
                        Partitions: Sub-divisions of topics for scalability and fault tolerance. Each partition is an ordered sequence of messages.
                    Consumers: Entities or applications that read data from Kafka topics.
                        Consumer Groups: A group of consumers that work together to consume a topic. Each message is processed by only one consumer in the group.
                    ZooKeeper: A distributed configuration service used by Kafka to manage brokers and maintain metadata.
                </pre>
            </p>
            <h4>2. Message Flow in Kafka</h4>
            <p>
                Producer -> Kafka Topic (Partition) -> Broker -> Consumer <br><br>

                Producers send messages to Kafka Cluster. <br>
                Messages are stored in Partitions within Topics on various Brokers. <br>
                Consumers pull messages from these partitions, ensuring each message is only processed once per consumer group. <br>
            </p>
            <h4>3. Kafka Topic and Partitioning</h4>
            <p>
                Each topic is split into partitions:
                <pre>
                    Topic A:
                    Partition 0: Messages 1, 2, 3, ...
                    Partition 1: Messages 4, 5, 6, ...
                    Partition 2: Messages 7, 8, 9, ...
                </pre>
            </p>
            <h4>4. Replication and Fault Tolerance</h4>
            <p>
                <pre>
                    Partition Replication: Each partition has replicas across multiple brokers.
                        Leader: The primary replica that handles all reads and writes.
                        Followers: Additional replicas that replicate the data and take over if the leader fails.
                </pre>
            </p>
            <h4>5. Kafka Producer API</h4>
            <p>
                A simplified view:
                <pre>
                    Producer:
                    Sends data -> Kafka Cluster
                    Uses acks (acknowledgments) to ensure message delivery.
                        acks=0: No acknowledgment
                        acks=1: Leader acknowledgment
                        acks=all: All replicas acknowledgment
                </pre>
            </p>
            <h4>6. Kafka Consumer API</h4>
            <p>
                A simplified view:
                <pre>
                    Consumer:
                        Reads data from Kafka Cluster.
                        Maintains offsets (position in the partition) to track read messages.
                        Part of a Consumer Group for load balancing and parallel processing.
                </pre>
            </p>
            <h4>7. Kafka Streams API</h4>
            <p>
                Illustrate how Kafka Streams processes data in real-time:
                <pre>
                    Streams Processor:
                        Consumes data from one or more topics.
                        Applies transformations (map, filter, aggregate).
                        Produces results to output topics.
                </pre>
            </p>
            <h4>8. Schema Registry and Serialization</h4>
            <p>
                <pre>
                    Producer -> Serializer (e.g., Avro, JSON, Protobuf) -> Kafka Topic
                    Consumer <- Deserializer <- Schema Registry (ensures compatibility)
                </pre>
            </p>
            <h4>9. Use Case Examples</h4>
            <p>
                Scenarios where Kafka is used:
                <pre>
                    Real-time Data Processing: Collecting and processing log data.
                    Event Sourcing: Storing events for later reprocessing.
                    Stream Processing: Real-time analytics and transformations.
                </pre>
            </p>
        </p>
        <h3>Apache Kafka Connect</h3>
        <p>
            <h4>1. Kafka Connect Architecture Overview</h4>
            <p>
                Kafka Connect Cluster: The core system for managing connectors and workers. <br>
                Connectors: Plugins that manage data transfer between Kafka and other systems. <br>
                <ul>
                    <li><strong>Source Connectors:</strong> Import data from external systems into Kafka topics.</li>
                    <li><strong>Sink Connectors:</strong> Export data from Kafka topics to external systems.</li>
                </ul>
                Workers: Instances of Kafka Connect that execute the connectors. <br>
                <ul>
                    <li><strong>Standalone Mode:</strong> Single worker instance, suitable for development and testing.</li>
                    <li><strong>Distributed Mode:</strong> Multiple worker instances for load balancing and fault tolerance.</li>
                </ul>
            </p>

            <h4>2. Data Flow in Kafka Connect</h4>
            <p>
                Source System -> Source Connector -> Kafka Topic -> Sink Connector -> Target System <br><br>
                Visualize the data flow through connectors and workers.
            </p>

            <h4>3. Components in Detail</h4>
            <p>
                <strong>Connectors:</strong> <br>
                <ul>
                    <li><strong>Source Connector:</strong> Reads data from a source system (e.g., a database, file system, or an application) and writes it to a Kafka topic.</li>
                    <li><strong>Sink Connector:</strong> Reads data from a Kafka topic and writes it to a target system (e.g., a database, file system, or another application).</li>
                </ul>
                <strong>Workers:</strong> <br>
                <ul>
                    <li><strong>Standalone Worker:</strong> A single process that runs both source and sink connectors.</li>
                    <li><strong>Distributed Workers:</strong> Multiple processes that share the workload of connectors, providing scalability and fault tolerance.</li>
                </ul>
            </p>

            <h4>4. Connector Configuration</h4>
            <p>
                Connector Configuration: Defines how data is read from or written to external systems. <br>
                Contains properties like connection details, data format, and transformation rules.
            </p>

            <h4>5. Task Distribution</h4>
            <p>
                In a distributed setup, the data processing tasks are split among workers: <br>
                <ul>
                    <li><strong>Worker 1:</strong> Executes Task 1 (Source Connector Part), Executes Task 2 (Sink Connector Part)</li>
                    <li><strong>Worker 2:</strong> Executes Task 3 (Source Connector Part), Executes Task 4 (Sink Connector Part)</li>
                </ul>
            </p>

            <h4>6. Offset Management</h4>
            <p>
                Kafka Connect manages the state of data processing using offsets: <br>
                <ul>
                    <li><strong>Source Connector:</strong> Tracks the position in the source system to ensure data is not missed or duplicated.</li>
                    <li><strong>Sink Connector:</strong> Tracks the position in Kafka topics to ensure data is processed sequentially.</li>
                </ul>
            </p>

            <h4>7. Transformations</h4>
            <p>
                Kafka Connect can apply transformations to data as it passes through: <br>
                <ul>
                    <li><strong>Single Message Transforms (SMTs):</strong> Modify individual messages (e.g., filter, rename fields, change formats) before they reach the destination.</li>
                </ul>
            </p>

            <h4>8. Monitoring and Management</h4>
            <p>
                Kafka Connect provides tools for monitoring and managing connectors and workers: <br>
                <ul>
                    <li><strong>REST API:</strong> Allows management of connectors (start, stop, configure) and workers. Provides metrics and status information.</li>
                    <li><strong>UI Tools:</strong> Many Kafka distributions include web interfaces for managing Kafka Connect.</li>
                </ul>
            </p>

            <h4>9. Error Handling</h4>
            <p>
                Kafka Connect includes mechanisms for handling errors: <br>
                <ul>
                    <li><strong>Dead Letter Queues:</strong> Messages that cannot be processed are sent to a separate Kafka topic for later analysis.</li>
                    <li><strong>Retry Policies:</strong> Configurations that determine how many times a connector should retry a failed operation before giving up.</li>
                </ul>
            </p>
        </p>
        <h3>Integration</h3>
        <p>
            System parts which could be integrated with Kafka are called source 
            - place from where come to the Kafka, - or sink -- place where data 
            from Kafka are mirrored. <br><br>
            
            Kafka Connect allows to automatically import and export data by 
            defining just a configuration file which looks like this:
            <pre>
                {
                    "connector.class": "io.confluent.connect.jdbc.JdbcSinkConnector",
                    "tasks.max": "1",
                    "topics": "Topic_Name",
                    "connection.url": "jdbc:postgresql://postgres:5432/db_name",
                    "connection.user": "username",
                    "connection.password": "pwd",
                    "auto.create": "true",
                    "insert.mode": "upsert",
                    "pk.mode": "record_value",
                    "pk.fields": "flight,flight_booking_class",
                    "driver.class": "org.postgresql.Driver",
                    "plugin.path": "/usr/share/java,/usr/share/confluent-hub-components",
                    "errors.log.enable": "true",
                    "errors.log.include.messages": "true"
                  }'
            </pre>

            The file is a self-explanatory, but it worth to comment a few things. <br><br>

            Topics property represents storage name where data in Kafka exist. 
            You might think about it as a table\tables names. <br>
            PK fields are primary keys of the tables data from Kafka goes to. <br><br>
            
            Personally I found importance of defining CLASSPATH here. Confluent 
            implementation of Kafka still relies on CLASSPATH in some cases, 
            despite on it is a legacy mechanism. <br><br>

            Besides config of integration with sink\source databases Kafka 
            Connect should know Kafka backend endpoint, advertise IP address 
            which others could reach this service and amount of replicas for 
            key datasets which should be aligned to the number of brokers in 
            system. Here is how Kafka Connect is defined as a docker service: 
            <pre>
                connect:
                    image: confluentinc/cp-kafka-connect:6.2.0
                    container_name: connect
                    hostname: connect
                    depends_on:
                        - kafka
                        - postgres
                    ports:
                        - "8083:8083"
                    environment:
                        CONNECT_BOOTSTRAP_SERVERS: "kafka:9092"
                        CONNECT_REST_ADVERTISED_HOST_NAME: 172.16.254.4
                        CONNECT_GROUP_ID: connect-cluster
                        CONNECT_CONFIG_STORAGE_TOPIC: _connect-configs
                        CONNECT_OFFSET_STORAGE_TOPIC: _connect-offsets
                        CONNECT_STATUS_STORAGE_TOPIC: _connect-status
                        CONNECT_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
                        CONNECT_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
                        CONNECT_PLUGIN_PATH: /usr/share/java,/usr/share/confluent-hub-components
                        CONNECT_REPLICATION_FACTOR: 1
                        CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
                        CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
                        CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
                        CLASSPATH: /usr/share/java/postgresql-42.2.23.jar:/usr/share/confluent-hub-components/kafka-connect-jdbc-10.7.6.jar
                    volumes:
                        - ./kafka-connect/connectors:/usr/share/confluent-hub-components
                        - ./kafka-connect/config:/etc/kafka-connect
                        - ./kafka-connect/postgresql-42.2.23.jar:/usr/share/java/postgresql-42.2.23.jar
                    networks:
                        priv-net:
                            ipv4_address: 172.16.254.4
            </pre>

            Kafka's service important env properties include broker id, endpoint 
            to communicate with a zookeeper, advertising endpoints for internal 
            and external communication with Kafka's service. Here is how it is 
            defined it docker service:
            <pre>
                kafka:
                    image: confluentinc/cp-kafka:6.2.0
                    hostname: kafka
                    container_name: kafka
                    depends_on:
                        - zookeeper
                    ports:
                        - "9092:9092"
                    environment:
                        KAFKA_BROKER_ID: 1
                        KAFKA_ZOOKEEPER_CONNECT: "zookeeper:2181"
                        KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
                        KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://172.16.254.3:9092
                        KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
                        KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
                        KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
                    networks:
                        priv-net:
                            ipv4_address: 172.16.254.3
                    healthcheck:
                        test: ["CMD", "sh", "-c", "exec nc -z localhost 9092"]
                        interval: 10s
                        timeout: 10s
                    retries: 5
            </pre> 

            <pre>KRaft</pre> is a preferable mechanism to use instead of 
            <pre>Zookeeper</pre>, although the current Kafka release 
            (apache/kafka:3.7.0) still rely on <pre>Zookeeper</pre>. We have to 
            use <pre>Zookeeper</pre> right now, but in the next release 
            <pre>Kafka 4.0</pre> it is going to be removed. Unfortunately, 
            in apache/kafka I faced with some bizarre issue related to 
            conflict between Zookeeper and KRaft <a href="https://stackoverflow.com/questions/78472810/how-to-run-pure-kafka-and-kafka-connect-over-docker-compose#78472810 ">which I have not resolved yet</a>. 
            This caused pivot of the project towards docker images from 
            Confluent Inc, they are under acceptable open source license, so 
            it is safe to use it. Here is Zookeeper docker service:
            <pre>
                zookeeper:
                    image: confluentinc/cp-zookeeper:6.2.0
                    hostname: zookeeper
                    container_name: zookeeper
                    ports:
                        - "2181:2181"
                    environment:
                        ZOOKEEPER_CLIENT_PORT: 2181
                        ZOOKEEPER_TICK_TIME: 2000
                    networks:
                        priv-net:
                            ipv4_address: 172.16.254.2
            </pre> 

            The rest docker services are <a href="https://github.com/Gelassen/Aeroflot/blob/main/pilot/docker-compose.yaml">available</a>
            in project's repository.
        </p>
        <h3>Backend</h3>
        <p>
            Backend doesn't have something special to share and serves as a 
            REST endpoint to view data. It is available in repository by 
            this <a href="https://github.com/Gelassen/Aeroflot/tree/main/pilot/REST">link</a>. <br><br>
            
            It is also packed in the docker and deployed as a service over a 
            docker compose.
        </p>
        <h4>Data producer</h4>
        <p>
            This project uses a PostgreSQL as a sink storage for data from Kafka, 
            but there is no source database. Instead another service called 
            producer writes this data to Kafka topic. <br><br>

            Integration with Kafka is done over special library and looks like 
            this:
            <pre>
                self.producer = KafkaProducer(
                    bootstrap_servers=[f"{self.kafka_host}:{self.kafka_port}"],
                    value_serializer=self.serializer
                )

                new_event = generate_inventory()
                self.producer.send(self.kafka_topic, new_event).add_callback(self.on_send_success).add_errback(self.on_send_error)
            </pre>

            Worth to note business specific aspect of the data. Airline 
            inventory data looks like this:
            <pre>
                "schema": {
                    "type": "struct",
                    "fields": [
                    {"type": "int64", "optional": False, "field": "time"},
                    {"type": "string", "optional": False, "field": "flight"},
                    {"type": "int64", "optional": False, "field": "departure"},
                    {"type": "string", "optional": False, "field": "flight_booking_class"},
                    {"type": "int32", "optional": False, "field": "idle_seats_count"}
                    ],
                    "optional": False,
                    "name": "InventorySchema"
                },
            </pre>

            Flight booking class has 25 (!) possible variants:
            <pre>
                flight_booking_class = [
                    "F", "A", "P", "R", "I", "D", "Z", "C", "J", "W", 
                    "S", "Y", "B", "H", "K", "L", "M", "N", "Q", "T", 
                    "V", "G", "X", "E", "U" 
                    ]
            </pre>

            I don't remember what each of them means, but every type represents 
            a unique business offer which airline prepare for their customers 
            which direct impact on the final cost of the airplane ticket. <br><br>
            
            The full code is available under this <a href="https://github.com/Gelassen/Aeroflot/tree/main/pilot/producer">link</a> 
        </p>
    </body>
</html>